# ğŸ—ï¸ Stalled Project News â€” Searchâ€‘First, Evidenceâ€‘Bound Project Updates (India)

A localâ€‘first pipeline that **searches the web (via SerpAPI)**, fetches only from **strictly whitelisted sources**, extracts evidence snippets, builds an event timeline, and generates a **humanâ€‘written style project update (500â€“1000 words)** with **hard noâ€‘hallucination guardrails**.

> âœ… Core rule: **Every factual claim must map to stored evidence** (snippet + source ref).  
> âŒ If evidence is missing, the output must say **â€œInsufficient evidenceâ€** instead of guessing.

---

## ğŸ§© Problem statement

Indian realâ€‘estate buyers/investors often hear *â€œproject is delayed / stuck / stalledâ€* but struggle to answer basic questions quickly:

- What **actually happened** (with dates)?
- Is there any **regulator/causeâ€‘list / court / credible news** mention?
- What is the **latest known update**?
- What does it mean for **buyers vs investors**?

Most online results are noisy: broker pages, duplicates, irrelevant PDFs, and generic claims. We needed a pipeline that is:

- **Searchâ€‘first** (no dependency on a single data source)
- **Whitelisted** (only trusted domains)
- **Evidenceâ€‘bounded** (no madeâ€‘up facts)
- **Repeatable** (stored artifacts per run, reproducible output)

---

## âœ… Solution (what this project does)

This project builds a deterministic pipeline:

1. **Query generation â†’ SERP retrieval (SerpAPI)**
2. **Whitelist filtering** (domain + optional subdomain rules)
3. **Fetch & extract** (store raw text per URL)
4. **Event/claim extraction** (dateâ€‘anchored events from extracted text)
5. **Deâ€‘dup + timeline build**
6. **News object generation** (OpenAI JSON mode; 500â€“1000 words; buyer + investor angle)
7. **Citation coverage verification** (refs used == refs in sources)
8. **Artifacts stored** for every run (JSON + HTML)

Outputs are written into an `artifacts/<slug>/<run_id>/` folder with everything needed to audit the result.

---

## ğŸ§  What the â€œmodelâ€ does (in plain terms)

This project uses an LLM only in the **final step** to write a readable narrative. The LLM:

- Receives a compact â€œdomain packâ€ + a strict timeline of events
- Must output a **JSON object only** (enforced via JSON response format)
- Is instructed to **never invent facts**
- Must cite only evidence refs that exist in the inputs

âœ… The â€œintelligenceâ€ comes from:
- Strong retrieval + whitelist control
- Evidence storage and event extraction that is snippetâ€‘backed
- Relevance gating to prevent timeline pollution

---

## ğŸ§± Tech stack

- ğŸ **Python** (tested on 3.10.x; compatible with 3.10+)
- ğŸ” **SerpAPI** (search results retrieval)
- ğŸŒ **HTTP fetching** (httpx / requests style fetchers)
- ğŸ“„ **Text extraction** (HTML â†’ text; optional PDF extraction where needed)
- ğŸ§  **OpenAI SDK** (JSON mode for strict structured output)
- ğŸ—‚ï¸ **Artifactsâ€‘first storage** (JSON/HTML files on disk)
- âœ… **CLI interface** (`python -m stalled_news ...`)
- ğŸ§¾ **YAML whitelist** (`configs/whitelist.yaml`)

---

## ğŸ“¦ Repository structure (high level)

```
stalled-project-news/
  src/stalled_news/
    __main__.py                 # CLI entrypoint
    models.py                   # ProjectInput + data models
    serp_pipeline.py            # SERP (basic)
    serp_wide_pipeline.py       # SERP (wide: adds news/general queries)
    evidence_pipeline.py        # Fetch + extract from serp_results.json
    event_extractor.py          # Dated event extraction + timeline storage
    news_generator.py           # Build news.json + news.html via OpenAI
    whitelist.py                # WhitelistPolicy + is_url_allowed
    whitelist_helpers.py        # YAML loader + policy construction
  configs/
    whitelist.yaml              # Allowed domains (add RERA/courts/news, etc.)
  artifacts/
    <project-slug>/<run-id>/    # Stored runs (serp_results, evidence, timeline, news)
```

---

## ğŸš€ Quickstart (endâ€‘toâ€‘end)

### 1) Setup
```bash
python -m venv .venv
source .venv/bin/activate
pip install -e .
```

### 2) Configure environment
Create `.env` (do **not** commit):
```bash
OPENAI_API_KEY=YOUR_KEY_HERE
OPENAI_MODEL=gpt-4.1-mini
SERPAPI_API_KEY=YOUR_SERPAPI_KEY
```

### 3) Add trusted domains (whitelist)
Edit:
```bash
configs/whitelist.yaml
```

Example:
```yaml
domains:
  - haryanarera.gov.in
  - maharera.mahaonline.gov.in
  - rera.karnataka.gov.in
  - rera.telangana.gov.in
  - www.rera.delhi.gov.in
  - indiankanoon.org
  - economictimes.indiatimes.com
  - livemint.com
  - squareyards.com
```

### 4) Run pipeline
#### A) Search
```bash
python -m stalled_news serp-run-wide   --project_name "ATS Grandstand"   --city "Gurgaon"
```

Youâ€™ll get:
```
stored: artifacts/<slug>/<run_id>/serp_results.json
whitelisted_results: <N>
```

#### B) Fetch + Extract
```bash
python -m stalled_news fetch-extract   --serp_results "artifacts/<slug>/<run_id>/serp_results.json"
```

Produces:
- `evidence.json`
- `/texts/*.txt` (one file per doc_id)

#### C) Extract Events
```bash
python -m stalled_news extract-events   --evidence "artifacts/<slug>/<run_id>/evidence.json"   --project_name "ATS Grandstand"   --city "Gurgaon"   --min_conf 0.55
```

Produces:
- `events_raw.json`
- `events_deduped.json`
- `timeline.json`

#### D) Render News (JSON + HTML)
```bash
python -m stalled_news render-news   --project_name "ATS Grandstand"   --city "Gurgaon"   --run_dir "artifacts/<slug>/<run_id>"   --events "events_deduped.json"
```

Produces:
- âœ… `news.json`
- âœ… `news.html`
- âœ… `news_inputs.json`
- âœ… `news_llm_raw.json`

---

## ğŸ§¾ Output schema (news.json)

The generator writes a strict schema like:

```json
{
  "headline": "string",
  "shortSummary": "2-3 lines",
  "detailedSummary": "500-1000 words",
  "primaryDateSource": {"date": "YYYY-MM-DD|null", "domain": "string|null", "ref": "doc_id", "url": "plain text"},
  "timeline": [{"date": "YYYY-MM-DD", "event": "string", "ref": "doc_id"}],
  "latestUpdate": {"date": "YYYY-MM-DD|null", "update": "string", "ref": "doc_id"},
  "buyerImplications": ["..."],
  "investorImplications": ["..."],
  "newsCoverage": [{"title": "string", "date": "YYYY-MM-DD|null", "sourceDomain": "string", "ref": "doc_id"}],
  "sources": [{"ref": "doc_id", "domain": "string", "urlText": "plain text (no hyperlink)"}],
  "generatedAt": "ISO",
  "validUntil": "ISO"
}
```

---

## ğŸ§ª Why timelines got polluted earlier (root cause) + fix âœ…

### Root cause
Some whitelisted regulator PDFs (ex: **causeâ€‘list PDFs**) contain **many unrelated cases** inside one document. The extractor was pulling dates from those PDFs and incorrectly attributing them to the target project, causing:
- unrelated events (other project names)
- very old dates
- junk domains

### Fix applied
We introduced **eventâ€‘level relevance gating**:
- Each candidate event must pass relevance checks using:
  - project name / key tokens
  - city (optional)
  - RERA id (if available)
- We reject events that donâ€™t mention the project context within the same text window.
- Evidence packing in the news generator is now **relevanceâ€‘filtered**, so the LLM sees fewer but higherâ€‘signal sources.

âœ… Result: timelines are driven by **projectâ€‘relevant** evidence only.

---

## ğŸ§­ Stepwise evolution (what we built, versionâ€‘wise)

### âœ… Step 6E â€” Wide SERP + robust fetch/extract
- Added â€œwideâ€ search mode: more query variants (news + general)
- Stored SERP artifacts (all results + whitelisted + domain summary)
- Fetch & extract pipeline writes `evidence.json` + extracted text files

### âœ… Step 6F â€” Event extraction + timeline artifacts
- Extracted dateâ€‘anchored events from stored texts
- Deâ€‘duped similar events
- Stored `events_raw.json`, `events_deduped.json`, `timeline.json`

### âœ… Step 6G â€” Render news (OpenAI JSON mode + HTML)
- Generated:
  - `news.json` (structured)
  - `news.html` (readable)
  - `news_inputs.json` (debug)
- Enforced â€œJSONâ€‘onlyâ€ output from the LLM

### âœ… Fix wave â€” Compatibility + correctness hardening
- Compatibility fixes across models / imports
- Stable evidence format handling (`docs` wide format â†’ compat list)
- WhitelistPolicy updated to support `allow_subdomains_for`
- **Relevance gating** to prevent unrelated PDFs polluting timelines
- Evidence packing reworked to avoid irrelevant domain packs
- CLI upgraded: extract-events accepts `--project_name --city --rera_id`

---

## ğŸ” Security & repo hygiene

- Never commit `.env`
- Keep `.env.example` with placeholders only:
  - `OPENAI_API_KEY=REPLACE_WITH_YOUR_OPENAI_KEY`
- If GitHub blocks pushes due to secret scanning:
  - **rewrite history** using `git filter-repo`

---

## ğŸ§° Handy snippets

### Check refs coverage (news.json)
```bash
python - <<'PY'
import json
from pathlib import Path
run_dir = Path("artifacts/<slug>/<run_id>")
news = json.loads((run_dir/"news.json").read_text())
used=set()
def collect(x):
    if isinstance(x, dict):
        for k,v in x.items():
            if k=="ref" and isinstance(v,str): used.add(v)
            collect(v)
    elif isinstance(x, list):
        for i in x: collect(i)
collect(news)
sources = {s.get("ref") for s in (news.get("sources") or []) if isinstance(s,dict)}
print("refs_used:", len(used))
print("missing_in_sources:", sorted([r for r in used if r not in sources])[:20])
PY
```

### Grep your extracted texts for a date or RERA id
```bash
RUN="artifacts/<slug>/<run_id>"
rg -n "GGM/582/314/2022/57|27[./-]06[./-]2022" "$RUN/texts" | head
```

---

## ğŸ—ºï¸ Roadmap ideas (next)
- ğŸ” Multiâ€‘project batch runner
- ğŸ§  Better PDF segmentation (split large cause lists into perâ€‘case slices)
- ğŸ§ª Automated eval suite (hallucination checks + coverage tests)
- ğŸŒ FastAPI wrapper for â€œgenerate on demandâ€ (optional)

---

## ğŸ‘¤ Author
Built by **Deepesh Gupta** â€” product + AI systems for realâ€‘estate discovery, trust, and intelligence. ğŸš€
